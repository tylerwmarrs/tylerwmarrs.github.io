<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Tyler Marrs (Posts about data analytics)</title><link>http://tylermarrs.com/</link><description></description><atom:link href="http://tylermarrs.com/categories/data-analytics.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2020; Tyler Marrs</copyright><lastBuildDate>Fri, 27 Mar 2020 00:45:01 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Interpreting Correlations</title><link>http://tylermarrs.com/posts/interpreting-correlations/</link><dc:creator>Tyler Marrs</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;This is a blog post that illustrates how to interpret different correlations. I have needed to explain this to many people and thought that it would be useful for others. Here is an embeded IPython notebook with explanation.&lt;/p&gt;
&lt;iframe src="http://tylermarrs.com/ipython/Interpreting%20Correlations.html" style="width:100%;height:700px"&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;/p&gt;&lt;/div&gt;</description><category>data analysis</category><category>data analytics</category><category>statistic</category><category>statistics</category><guid>http://tylermarrs.com/posts/interpreting-correlations/</guid><pubDate>Mon, 01 May 2017 03:16:44 GMT</pubDate></item><item><title>Edit Distance Hierarchical Clustering</title><link>http://tylermarrs.com/posts/edit-distance-hierarchical-clustering/</link><dc:creator>Tyler Marrs</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;
    This is a quick blog post to illustrate how you can use Scipy, NLTK and Numpy to perform hierarchical clustering based on edit distance. Performing hierarchical clustering based on edit distance allows you to see what words or set of text is similar based on edit distance. Edit distance is the number of changes required to make text x turn into text y. Essentially, you could quickly look at a number of labels to check for typos or duplications.
&lt;/p&gt;
&lt;p&gt;
    Here is a code snippet illustrating how you can cluster some labels and create a dendrogram of the relationships.
&lt;/p&gt;

&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;maplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;nltk.metrics.distance&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;edit_distance&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.cluster.hierarchy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;linkage&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.cluster.hierarchy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;dendrogram&lt;/span&gt;

&lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="s1"&gt;'Tacos'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;'Tacoes'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;'Tomatos'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;'Tomatoes'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;'Beef'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;'Beer'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;'Steak'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;'Pig'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;'Twig'&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coord&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;coord&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;edit_distance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;coords&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;triu_indices&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply_along_axis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;coords&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;clust&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;linkage&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;dend&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dendrogram&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;clust&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;leaf_font_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;


&lt;p&gt;
    The dendrogram from this set of data looks like this:
&lt;/p&gt;

&lt;p style="text-align: center;"&gt;
    &lt;img alt="" class="size-full wp-image-243 aligncenter" height="251" src="http://tylermarrs.com/wp-content/uploads/2017/03/edit_distance_dedrogram.png" style="" title="" width="375"&gt;
&lt;/p&gt;

&lt;p&gt;
    The x axis shows the labels of text and the y axis shows the edit distance. The words "Beer" and "Beef" are clustered together as only one change must be made to make the words similar. The words "Pig" and "Twig" require two changes.
&lt;/p&gt;

&lt;p&gt;
    I thought that this was very useful in a scenario of data cleansing where there were many duplications of a given label. Automating the actual corrections of the text may depend on your use case, but visualizing these discrepencies up front can help you make a better decision.
&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;&lt;/div&gt;</description><category>clustering</category><category>data analytics</category><category>edit distance</category><category>nltk</category><category>numpy</category><category>scipy</category><guid>http://tylermarrs.com/posts/edit-distance-hierarchical-clustering/</guid><pubDate>Wed, 15 Mar 2017 20:55:35 GMT</pubDate></item><item><title>Lyric Analysis of an Artist</title><link>http://tylermarrs.com/posts/lyric-analysis-of-an-artist/</link><dc:creator>Tyler Marrs</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;
    Introduction
&lt;/h2&gt;
&lt;p&gt;
    This is a follow up post to the lyric analysis performed for the &lt;a href="http://tylermarrs.com/billboard-hot-100-lyric-analysis/"&gt;Billboard Hot 100 songs lyric analysis&lt;/a&gt; (referred to as - Billboard analysis). Within the Billboard analysis, I attempted to find trends within the lyrics that may influence why a particular song is popular. However, I did not find anything of interest. After no trends were found in the Billboard analysis, I wondered what I could find by analyzing every song performed by an artist. There is no particular trend of interest that I tried to find within an artist. This is purely an exploratory endeavor. All of the code associated with this pipeline can be found at the following &lt;a href="https://github.com/tylerwmarrs/single-artist-lyric-analysis" target="_blank"&gt;Github repository&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
    &lt;u&gt;&lt;strong&gt;Please take caution while reading this blog post. Some offensive language is used for analysis purposes.&lt;/strong&gt;&lt;/u&gt;
&lt;/p&gt;

&lt;h2&gt;
    Methods
&lt;/h2&gt;

&lt;p&gt;
    In this section you will find the methods used to collect, cleanse, process and visualize the data. The image below represents the tasks associated with the entire data pipeline. It is a DAG (directed acyclic graph) of every task in the pipeline (created with &lt;a href="https://github.com/spotify/luigi" target="_blank"&gt;Luigi&lt;/a&gt;).
&lt;/p&gt;

&lt;p style="text-align: center;"&gt;
    &lt;img alt="" class="size-full wp-image-208 aligncenter" height="356" src="http://tylermarrs.com/wp-content/uploads/2017/03/dependency_graph.png" style="" title="" width="571"&gt;
&lt;/p&gt;

&lt;p style="text-align: center;"&gt;
    &lt;em&gt;Figure 1. &lt;/em&gt;Data pipeline
&lt;/p&gt;

&lt;p&gt;
    The data pipeline takes an artist and an output directory as input. It then goes through the process of extracting lyrics, cleaning the lyrics, processing the lyrics and finally generating some visualizations. These processes are explained in further detail below.
&lt;/p&gt;

&lt;h3&gt;
    Data Collection
&lt;/h3&gt;

&lt;p&gt;
    The data collection process first searches for the artist provided as input on the site AZLyrics.com. If an artist shows in the search results, the pipeline further scrapes the website to obtain all songs available. These tasks, ObtainArtistUrl, FetchArtistSongs and FetchLyrics, are all performed using the &lt;a href="https://pypi.python.org/pypi/requests/2.13.0" target="_blank"&gt;Requests&lt;/a&gt; and &lt;a href="https://pypi.python.org/pypi/lxml/3.7.2" target="_blank"&gt;LXML&lt;/a&gt; libraries available in the Python language. Prior to web scraping, the pipeline creates a directory structure in the CreateDirectories task to store data, plots and reports in a tidy fashion within the specified output directory. Similar to the Billboard analysis, the swear word list from no-swearing.com is used to look for curse words throughout the text.
&lt;/p&gt;

&lt;h3&gt;
    Data Cleansing
&lt;/h3&gt;

&lt;p&gt;
    Preparation of the lyrics files consisted of removing unwanted text within the files. Many of the files contained labels in brackets to mark the chorus and other constructs of a song. These were removed from the files. Additionally, some lyrics files had a line that consisted of "Produced by…" which was removed. All data cleansing techniques used regular expressions in the Python programming language. Prior to this data cleansing step, raw data analysis was performed to determine what was causing "dirty results".
&lt;/p&gt;

&lt;h3&gt;
    Text Processing and Visualization
&lt;/h3&gt;

&lt;p&gt;
    The Python language and the following libraries were used to process and visualize the text: &lt;a href="http://www.nltk.org/" target="_blank"&gt;NLTK&lt;/a&gt;, &lt;a href="http://pandas.pydata.org/" target="_blank"&gt;Pandas&lt;/a&gt;, &lt;a href="http://matplotlib.org/" target="_blank"&gt;Matplotlib&lt;/a&gt; and &lt;a href="http://scikit-learn.org/stable/documentation.html" target="_blank"&gt;Scikit-learn&lt;/a&gt;. NLTK is a natural language processing tool kit that provides convenience functions for common tasks such as tokenization and sentiment analysis. The Pandas library is a tool that provides common data analysis features. It enables you to easily perform aggregations, obtain descriptive statistics and plot data by wrapping convenience methods from other libraries. When Pandas didn't provide the exact plotting styles that I wanted out of the box, I used Matplotlib functionality to add styling to the plots. Scikit-learn was used to extract TF-IDFs (term frequency–inverse document frequency) and to perform topic modeling. &lt;a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" target="_blank"&gt;TF-IDF&lt;/a&gt; is essentially a statistical measure of how important a word is within a given document. &lt;a href="https://en.wikipedia.org/wiki/Topic_model" target="_blank"&gt;Topic modeling&lt;/a&gt; is a method that tries to find semantic meanings within a set of given text. In this analysis we use two different topic modeling algorithms; &lt;a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation" target="_blank"&gt;LDA&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Non-negative_matrix_factorization" target="_blank"&gt;NMF&lt;/a&gt;.
&lt;/p&gt;

&lt;h2&gt;
    Analysis
&lt;/h2&gt;

&lt;p&gt;
    In this section we look at the output of the data pipeline for the artists Twenty One Pilots and Eminem. These two artists were chosen to illustrate potential insights that can be gained from artists. Additionally, these two artists come from two different genres; pop and rap.
&lt;/p&gt;

&lt;p&gt;
    Many of these techniques have already been explained in the Billboard analysis and are left out for brevity. I encourage you to read that blog post to learn more about the techniques.
&lt;/p&gt;

&lt;h3&gt;
    Word Frequency
&lt;/h3&gt;

&lt;p&gt;
    The word frequency represents the most common words found throughout all of the lyrics. It is normalized by the number of words in each document and the total number of songs observed. Comparing the artists below, we can see that Eminem has a tendency to swear a lot while Twenty One Pilots does not.
&lt;/p&gt;

&lt;p style="text-align: center;"&gt;
    &lt;img alt="" class="alignnone size-full wp-image-218" height="330" src="http://tylermarrs.com/wp-content/uploads/2017/03/word_frequency_plot_twentyone_pilots.png" style="" title="" width="480"&gt;&lt;img alt="" class="alignnone size-full wp-image-219" height="330" src="http://tylermarrs.com/wp-content/uploads/2017/03/word_frequency_plot_eminem.png" style="" title="" width="480"&gt;
&lt;/p&gt;

&lt;h3&gt;
    Swear Word Frequency
&lt;/h3&gt;

&lt;p&gt;
    The swear word frequency is a measure of the number of curse words found throughout the lyrics. It is similar to the word frequency with the exception of curse words being targeted. The plots below only show the top 5 curse words found throughout the lyrics. It is pretty clear that Eminem curses quite a bit in his songs. The F-word shows up around 1.4% of the time. However, Twenty One Pilots only appears to curse very minimally with roughly 0.032% of the word "muff".
&lt;/p&gt;

&lt;p&gt;
    While we are only looking at the frequency of individual swear words, one might be interested in an overall frequency. For the purposes of this analysis it has been left out, but it could easily be calculated.
&lt;/p&gt;

&lt;p style="text-align: center;"&gt;
    &lt;img alt="" class="alignnone size-full wp-image-236" height="330" src="http://tylermarrs.com/wp-content/uploads/2017/03/swearword_frequency_plot_twentyone_pilots2.png" style="" title="" width="480"&gt;&lt;img alt="" class="alignnone size-full wp-image-223" height="330" src="http://tylermarrs.com/wp-content/uploads/2017/03/swearword_frequency_plot_eminem.png" style="" title="" width="480"&gt;
&lt;/p&gt;

&lt;h3&gt;
    Repetitiveness
&lt;/h3&gt;

&lt;p&gt;
    The repetitiveness is a measure of how many repetitions a particular phrase occurs. A threshold of 2 occurrences must be met in order for it to be considered repetitive. The actual calculation can be read within the Billboard analysis. The plots below show that Twenty One Pilots has a very high average repetition rate of 52% while Eminem has a much lower average rate of 7%.
&lt;/p&gt;

&lt;p style="text-align: center;"&gt;
    &lt;img alt="" class="alignnone size-full wp-image-224" height="330" src="http://tylermarrs.com/wp-content/uploads/2017/03/repetitiveness_plot_twentyone_pilots.png" style="" title="" width="480"&gt;&lt;img alt="" class="alignnone size-full wp-image-225" height="330" src="http://tylermarrs.com/wp-content/uploads/2017/03/repetitiveness_plot_eminem.png" style="" title="" width="480"&gt;
&lt;/p&gt;

&lt;h3&gt;
    Sentiment
&lt;/h3&gt;

&lt;p&gt;
    The sentiment algorithm used is Vader within the NLTK library. It is a rule based algorithm trained from Twitter data. The Twenty One Pilots plot shows that their songs consist of both positive and negative sentiments while Eminem's lyrics have a higher negative sentiment.
&lt;/p&gt;

&lt;p style="text-align: center;"&gt;
    &lt;img alt="" class="alignnone size-full wp-image-239" height="330" src="http://tylermarrs.com/wp-content/uploads/2017/03/sentiment_plot_twentyone_pilots.png" style="" title="" width="480"&gt;&lt;img alt="" class="alignnone size-full wp-image-240" height="330" src="http://tylermarrs.com/wp-content/uploads/2017/03/sentiment_plot_eminem.png" style="" title="" width="480"&gt;
&lt;/p&gt;

&lt;h3&gt;
    Song Statistics
&lt;/h3&gt;

&lt;p&gt;
    The song statistics simply plots the repetitiveness, positive sentiment and negative sentiment per song.
&lt;/p&gt;

&lt;p style="text-align: center;"&gt;
    &lt;img alt="" class="alignnone size-full wp-image-226" height="330" src="http://tylermarrs.com/wp-content/uploads/2017/03/song_statistics_plot_twentyone_pilots.png" style="" title="" width="480"&gt;&lt;img alt="" class="alignnone size-full wp-image-227" height="330" src="http://tylermarrs.com/wp-content/uploads/2017/03/song_statistics_plot_eminem.png" style="" title="" width="480"&gt;
&lt;/p&gt;

&lt;h3&gt;
    Album Statistics
&lt;/h3&gt;

&lt;p&gt;
    The album statistics plots the repetitiveness, positive sentiment and negative sentiment per song. It is interesting that Twenty One Pilots highest repetitive album is Blurryface at a whopping 68%. The most repetitive album from Eminem is Straight From The Vault at around 23%.
&lt;/p&gt;

&lt;p style="text-align: center;"&gt;
    &lt;img alt="" class="alignnone size-full wp-image-228" height="330" src="http://tylermarrs.com/wp-content/uploads/2017/03/album_statistics_plot_twentyone_pilots.png" style="" title="" width="480"&gt;&lt;img alt="" class="alignnone size-full wp-image-229" height="330" src="http://tylermarrs.com/wp-content/uploads/2017/03/album_statistics_plot_eminem.png" style="" title="" width="480"&gt;
&lt;/p&gt;

&lt;h3&gt;
    Topic Modeling
&lt;/h3&gt;

&lt;p&gt;
    The topic models shown below (one topic per line) is not very human friendly, however it makes sense to the computer. You can decipher a little bit of information from reading the topics. To build these topic models I used Sklearn. It extracted Ngrams from 1 to 2 N using the TfidfVectorizer within Sklearn and was fed into the algorithms. Essentially the vectorizer provides you with a list of features (Ngrams) and their inverse document frequency.
&lt;/p&gt;

&lt;p&gt;
    You can see that LDA and NMF give different results. LDA is typically used in documents where there are semantic units within the text while NMF is used for non-word based documents such as images.
&lt;/p&gt;

&lt;h4&gt;
    LDA
&lt;/h4&gt;

&lt;p&gt;
    &lt;strong&gt;Twenty One Pilots Topics&lt;/strong&gt;
&lt;/p&gt;

&lt;pre&gt;
&lt;code&gt;na na na ll tell reign
da hey hey hey trying trying trying
bah re broken bah bah broken doo
fall don heart won watch
save re alive stay alive eh eh&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;
    &lt;strong&gt;Eminem Topics&lt;/strong&gt;
&lt;/p&gt;

&lt;pre&gt;
&lt;code&gt;don cause shit fuck re
daddy phenomenal wanna love alright am phenomenal
paul rape don fuckin kidding ummm listened dead gay
yo requested intent checking week sayin called ok retilin goes
iâ canâ fucking kidding whatâ ish&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;
    NMF
&lt;/h4&gt;

&lt;p&gt;
    &lt;strong&gt;Twenty One Pilots Topics&lt;/strong&gt;
&lt;/p&gt;

&lt;pre&gt;
&lt;code&gt;tell ll tell plans ll tell plans
na na na hello hello hello silent trees
hey hey hey trying trying trying trying sleep
sit sit silence silence car radio sit
play pretend pretend wish stressed re stressed&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;
    &lt;strong&gt;Eminem Topics&lt;/strong&gt;
&lt;/p&gt;

&lt;pre&gt;
&lt;code&gt;don cause shit fuck re
name name name huh hi yeah yeah
paul ummm listened paul ummm incest incest rape
woah alright drug woah woah steve
daddy mommy rejoice carry don voice looking&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;
    Conclusion
&lt;/h2&gt;

&lt;p&gt;
    Building this data pipeline has proved to provide me with useful information about a particular artist without actually listening to any of their songs. One could use some of these techniques to identify music suited for a particular age or culture. For example, you may not want your kid to listen to songs with a high rate of swearing or negativity. Creating this type of intuition about a particular artist may enable individuals to quickly compare several artists at once with some adjustments to the pipeline. Some unsupervised clustering could be used in conjunction with the topic models to find similar artists based on lyrics.
&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;&lt;/div&gt;</description><category>data analytics</category><category>data mining</category><category>matplotlib</category><category>nlp</category><category>pandas</category><category>python</category><category>text analysis</category><guid>http://tylermarrs.com/posts/lyric-analysis-of-an-artist/</guid><pubDate>Mon, 13 Mar 2017 09:11:00 GMT</pubDate></item><item><title>Billboard Hot 100 Lyric Analysis</title><link>http://tylermarrs.com/posts/billboard-hot-100-lyric-analysis/</link><dc:creator>Tyler Marrs</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;
    Introduction
&lt;/h2&gt;
&lt;p&gt;
    The Billboard Hot 100, from &lt;a href="http://billboard.com" target="_blank"&gt;billboard.com&lt;/a&gt;, is a list of rankings for the most popular 100 songs of a given week. The website claims to establish their rankings by "...fan interactions with music, including album sales and downloads, track downloads, radio airplay and touring as well as streaming and social interactions on Facebook, Twitter, Vevo, Youtube, Spotify and other popular online destinations for music." (billboard.com, 2017).
&lt;/p&gt;

&lt;p&gt;
    In this analysis the goal is to see if any trends appear within the song's lyrics through text analysis. This project was created purely out of personal interest to see if any lyrical patterns exist between popular songs. All of the code and data used during this analysis can be viewed at the following &lt;a href="https://github.com/tylerwmarrs/billboard-hot-100-lyric-analysis" target="_blank"&gt;GitHub repository&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
    &lt;u&gt;&lt;strong&gt;Please take caution while reading this blog post. Some offensive language is used for analysis purposes.&lt;/strong&gt;&lt;/u&gt;
&lt;/p&gt;

&lt;h2&gt;
    Methods
&lt;/h2&gt;

&lt;p&gt;
    In this section we explore the methods and tools used to extract data from sources, clean the data and analyze the text.
&lt;/p&gt;

&lt;h3&gt;
    Data Collection
&lt;/h3&gt;

&lt;p&gt;
    The first task at hand was to determine which data sources provided the song list and the lyrics. Thankfully, billboard.com provided an RSS feed that could easily be parsed to process the top 100 songs for a given week while obtaining the lyrics was much more difficult. To parse the RSS feed, I used the Python programming language and the software libraries: &lt;a href="https://pypi.python.org/pypi/lxml/3.7.2" target="_blank"&gt;LXML&lt;/a&gt; and &lt;a href="https://pypi.python.org/pypi/requests/2.13.0" target="_blank"&gt;Requests&lt;/a&gt;. LXML provides functionality to process DOM elements on web pages (HTML and XML). The Requests library enables you to create HTTP requests to obtain web content. 
&lt;/p&gt;

&lt;p&gt;
    The billboard.com RSS feed was obtained by making a GET request with the Request's library and further parsed using LXML. For every song in the RSS feed, the artist, title, rank this week, rank last week was collected. During the iteration of each RSS entry, a request to a lyric site was made to obtain the lyrics (discussed below). Once processed, a comma-delimited file was created to save the song entries. A simple illustration of this process is shown below.
&lt;/p&gt;

&lt;p style="text-align: center;"&gt;
    &lt;img alt="" class="alignnone size-full wp-image-172" height="252" src="http://tylermarrs.com/wp-content/uploads/2017/02/lyrics-collection-workflow-2.png" width="712"&gt;
&lt;/p&gt;

&lt;p&gt;
    Searching for websites that provided lyrics was the longest process in this whole project. There are several lyrics websites available, however many of them are community driven and do not always have every song's lyrics at hand. That created a challenge in creating a good combination of many different web sources to obtain lyrics from. &lt;a href="http://azlyrics.com" target="_blank"&gt;AZLyrics.com&lt;/a&gt;, &lt;a href="http://songlyrics.com" target="_blank"&gt;SongLyrics.com&lt;/a&gt; and &lt;a href="http://lyricsfreak.com" target="_blank"&gt;LyricsFreak.com&lt;/a&gt; were all used to obtain lyrics. The process consisted of using the Requests and LXML libraries in the Python programming language. Essentially, the script would create a search request on each site until it found lyrics it was looking for. Once the lyrics were found it would extract them from the web page and save them to a text file. If one website did not have the lyrics, it would try the next one. If none of the sites contained the lyrics, it would raise an error. In the case of an error, manual web searching is required to obtain the lyrics. To create the file name for the lyrics file, I used the Python library &lt;a href="https://pypi.python.org/pypi/unicode-slugify/0.1.3" target="_blank"&gt;unicode-sluggify&lt;/a&gt;.This library "sluggifies" the provided text replacing spaces with dashes and other symbols accordingly. It is normally used in creating "pretty" URLs.
&lt;/p&gt;

&lt;p&gt;
    In addition to the top songs and lyrics, I collected a set of swear words from &lt;a href="http://noswearing.com" target="_blank"&gt;noswearing.com&lt;/a&gt;. NoSwearing.com provides a list of swear words that are both common and uncommon. To extract these swear words I used the same libraries (Requests and LXML) to scrape data from the website. The website is laid out with all swear words separated alphabetically on their own web page. The script used to scrape the words iterated through the alphabet constructing the URL accordingly and fetched the swear words saving them to a text file.
&lt;/p&gt;

&lt;h3&gt;
    Data Cleansing
&lt;/h3&gt;

&lt;p&gt;
    Preparation of the lyrics files consisted of removing unwanted text within the files. Many of the files contained labels in brackets to mark the chorus and other constructs of a song. These were removed from the files. Additionally, some lyrics files had a line that consisted of "Produced by..." which was removed. All data cleansing techniques used regular expressions in the Python programming language. Prior to this data cleansing step, raw data analysis was performed to determine what was causing "dirty results".
&lt;/p&gt;

&lt;h3&gt;
    Text Processing and Visualization
&lt;/h3&gt;

&lt;p&gt;
    The Python language and the following libraries were used to process and visualize the text: &lt;a href="http://www.nltk.org/" target="_blank"&gt;NLTK&lt;/a&gt;, &lt;a href="http://pandas.pydata.org/" target="_blank"&gt;Pandas&lt;/a&gt; and &lt;a href="http://matplotlib.org/" target="_blank"&gt;Matplotlib&lt;/a&gt;. NLTK is a natural language processing tool kit that provides convenience functions for common tasks such as tokenization and sentiment analysis. The Pandas library is a tool that provides common data analysis features. It enables you to easily perform aggregations, obtain descriptive statistics and plot data by wrapping convenience methods from other libraries. When Pandas didn't provide the exact plotting styles that I wanted out of the box, I used Matplotlib functionality to add styling to the plots.
&lt;/p&gt;

&lt;h2&gt;
    Analysis
&lt;/h2&gt;

&lt;p&gt;
    In this section you will find the different analyses performed on the lyrics.
&lt;/p&gt;

&lt;h3&gt;
    Word Frequency
&lt;/h3&gt;

&lt;p&gt;
    One of the first analyses performed was to determine the frequency of words throughout the lyrics. This process was performed by tokenizing each of the words with NLTK, removing the stop words, stemming words with PorterStemmer, collecting word frequencies per lyrics file and normalizing the word frequency. Normalization was performed by treating each set of lyrics individually and collecting the frequency that each word showed up in the lyrics file individually. Once a frequency was obtained for each set of lyrics, a global frequency was calculated by combining all local frequencies and dividng by 100 (the number of songs). This method essentially treats every lyrics file as a "bag of words instead of all lyrics text combined as a "bag of words".
&lt;/p&gt;

&lt;p style="text-align: center;"&gt;
    &lt;img alt="" class="size-full wp-image-159 aligncenter" height="268" src="http://tylermarrs.com/wp-content/uploads/2017/02/word_frequency_normalized.png" style="" title="" width="362"&gt;
&lt;/p&gt;

&lt;p&gt;
    The bar chart above shows some of the common words used throughout the songs. The Y-axis is the frequency in a decimal percentage while the X-axis displays the words. So the word "love" shows up 1.5% of the time in all of the songs. Looking at the word frequencies could give you a general idea that "love" is a fairly popular topic in this set of songs.
&lt;/p&gt;

&lt;h3&gt;
    Swear Words
&lt;/h3&gt;

&lt;p&gt;
    The next analysis performed was to look at how frequent swear words occurred in the songs. The same process used to obtain the word frequency was used to calculate the swear word frequency with one additional step - filtering the words of interest. 
&lt;/p&gt;

&lt;p style="text-align: center;"&gt;
    &lt;img alt="" class="size-full wp-image-156 aligncenter" height="269" src="http://tylermarrs.com/wp-content/uploads/2017/02/swear_word_frequency_normalized_stemmed.png" style="" title="" width="362"&gt;
&lt;/p&gt;

&lt;p&gt;
    As illustrated in the plot, there is a very low frequency of swear words found throughout these songs. The most frequent swear word has a frequency of 0.7%. I imagine this is due to the nature of how billboard.com collects their information to create their list or songs with many swear words just are not as popular. Speculation aside, this would require more analysis and data collection on this specific topic to reliably determine the case.
&lt;/p&gt;

&lt;h3&gt;
    N-Grams
&lt;/h3&gt;

&lt;p&gt;
    Similar to looking at common words in the songs, I looked for common N-Grams. An N-Gram is a contiguous set of text. In this case our N-Grams represent a number of words.
&lt;/p&gt;

&lt;p style="text-align: center;"&gt;
    &lt;img alt="" class="size-full wp-image-148 aligncenter" height="383" src="http://tylermarrs.com/wp-content/uploads/2017/02/common_trigrams.png" style="" title="" width="353"&gt;
&lt;/p&gt;

&lt;p&gt;
    The plot above shows the top TriGrams with their frequency.
&lt;/p&gt;

&lt;p style="text-align: center;"&gt;
    &lt;img alt="" class="size-full wp-image-147 aligncenter" height="317" src="http://tylermarrs.com/wp-content/uploads/2017/02/common_bigrams.png" style="" title="" width="353"&gt;
&lt;/p&gt;

&lt;p&gt;
    The plot above shows the top BiGrams with their frequency. You might notice that there isn't a big difference between looking at TriGrams versus BiGrams. The majority of the same results show as the top. 
&lt;/p&gt;

&lt;h3&gt;
    Sentiments
&lt;/h3&gt;

&lt;p&gt;
    To gain insight into the general sentiment of these popular songs, the &lt;a href="https://github.com/cjhutto/vaderSentiment" target="_blank"&gt;Vader sentiment analysis algorithm&lt;/a&gt; was used. This algorithm is a rule-based algorithm that was trained with Twitter data. It provides a good solution to handling slang and emoticons that isn't always seen in most algorithms. Another approach, given that I collectd more data, would be to create my own sentiment analysis tool using a NaiveBayes model. However, for this particular problem I felt that Vader would suffice. This is primarily due to no known source of lyrics with labeled sentiments.
&lt;/p&gt;

&lt;p&gt;
    I used the Vader algorithm on each individual line in a given lyrics file. The output of the algorithm is a probability score of neutrality, positivity and negativity. To obtain the sentiment for an entire song, each line was processed and the sum of each probability was gathered. Once all lines were processed, a normalized score was obtained by dividing by the total number of lines analyzed in the lyrics file. To obtain a global sentiment for all of the songs in the set, each song's sentimet scores were summed and divided by the total number of songs analyzed (100).
&lt;/p&gt;

&lt;p style="text-align: center;"&gt;
    &lt;img alt="" class="size-full wp-image-154 aligncenter" height="281" src="http://tylermarrs.com/wp-content/uploads/2017/02/sentiment_distribution.png" style="" title="" width="350"&gt;
&lt;/p&gt;

&lt;p&gt;
    In this plot you can see that the songs are generally scored as neutral, however there is a 2-3% difference between positive and negative sentiments. I imagine that the sentiment analysis tool, Vader, is most likely not optimal for lyric analysis. Experimentation with other algorithms would need to be explored to determine if a better solution is available.
&lt;/p&gt;

&lt;h3&gt;
    Repetitiveness
&lt;/h3&gt;

&lt;p&gt;
    Have you ever listened to a song that seemed as if it was saying the same thing over and over again? Typically these songs become "stuck in your head" so I wanted to measure how repetitive a given song is. A custom algorithm was used to measure repetitiveness. Essentially, all unique phrases (each individual line) was obtained for each lyric file and a frequency for each phrase was calculated. All unique phrases that had 2 or more repetitions was summed together and divided by the total number of phrases in the file. The result is a percentage of repetitive lines.
&lt;/p&gt;

&lt;p&gt;
    A different approach for measuring repetitiveness would be at the word level. However, some songs use words in different phrases that may not sound repetitive. Either approach would work, however I felt that measuring at the phrase level made more sense. 
&lt;/p&gt;

&lt;p style="text-align: center;"&gt;
    &lt;img alt="" class="size-full wp-image-181 aligncenter" height="466" src="http://tylermarrs.com/wp-content/uploads/2017/02/repetitiveness.png" style="" title="" width="925"&gt;
&lt;/p&gt;

&lt;p&gt;
    The plot above shows the percentage of repetitions found in each song. The songs from left to right are ordered by the 1st rank for the week to the 100th rank. You can also see that generally a lot of these songs are pretty repetitive with a median of nearly 58%. Althougth these songs may seem very repetitive, more data would need to be analyzed to get a better median and mean repetition percentage. In other words, it is difficult to say that these songs are very repetitive when we do not know how repetitive most songs are.
&lt;/p&gt;

&lt;h3&gt;
    Correlations
&lt;/h3&gt;

&lt;p&gt;
    I looked for trends between sentiments, current rank and repetitiveness. However, no strong correlations were found. The Pearson correlation was used to identify any potential correlations. Pandas provides a simple way to obtain all correlations in a table format between all numerical values. For illustration, I included some scatter plots.
&lt;/p&gt;

&lt;p style="text-align: center;"&gt;
    &lt;img alt="" class="alignnone size-full wp-image-188" height="195" src="http://tylermarrs.com/wp-content/uploads/2017/02/Repetitiveness_and_neg_sent.png" style="" title="" width="273"&gt; &lt;img alt="" class="alignnone size-full wp-image-189" height="195" src="http://tylermarrs.com/wp-content/uploads/2017/02/Repetitiveness_and_pos_sent.png" style="" title="" width="273"&gt; &lt;img alt="" class="alignnone size-full wp-image-190" height="195" src="http://tylermarrs.com/wp-content/uploads/2017/02/pos_sent_current_rank.png" style="" title="" width="273"&gt; &lt;img alt="" class="alignnone size-full wp-image-191" height="195" src="http://tylermarrs.com/wp-content/uploads/2017/02/neg_sent_current_rank.png" style="" title="" width="273"&gt; &lt;img alt="" class="alignnone size-full wp-image-192" height="195" src="http://tylermarrs.com/wp-content/uploads/2017/02/Repetitiveness_and_rank.png" style="" title="" width="272"&gt;
&lt;/p&gt;

&lt;h2&gt;
    Conclusion
&lt;/h2&gt;

&lt;p&gt;
    While no correlations were found within the lyrics, many useful techniques and insights were gained in our analysis. We found that within our samples swearing is fairly limited with less than a 1% frequency, songs contain generally neutral sentiment with similar measures of positivity and negativity and that there appears to be a fairly high number of repititions in songs. 
&lt;/p&gt;

&lt;p&gt;
    Some things to take note of is that all of these songs come from different genres and artists which makes finding trends more difficult. Also, it may not only be lyrics that make a song popular.
&lt;/p&gt;

&lt;p&gt;
    Here is a list of further analysis topics that could be interesting:
&lt;/p&gt;

&lt;ul&gt;
    &lt;li&gt;
        Temporal analysis of Billboard Hot 100 songs across a year's period.
    &lt;/li&gt;
    &lt;li&gt;
        All songs for a particular artist.
    &lt;/li&gt;
    &lt;li&gt;
        Many songs from a particular genre.
    &lt;/li&gt;
    &lt;li&gt;
        Many songs from multiple genres treated as separate cohorts.
    &lt;/li&gt;
    &lt;li&gt;
        Improve sentiment analysis by training our own model. This would be very complicated with no labels.
    &lt;/li&gt;
    &lt;li&gt;
        Combine a songs audio and lyrics to perform a similar analysis.
    &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;/p&gt;&lt;/div&gt;</description><category>data analytics</category><category>data mining</category><category>matplotlib</category><category>nlp</category><category>nltk</category><category>pandas</category><category>python</category><category>text analysis</category><guid>http://tylermarrs.com/posts/billboard-hot-100-lyric-analysis/</guid><pubDate>Fri, 17 Feb 2017 22:15:23 GMT</pubDate></item><item><title>Ranked Win Predictor</title><link>http://tylermarrs.com/posts/ranked-win-predictor/</link><dc:creator>Tyler Marrs</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;
    Abstract
&lt;/h3&gt;
&lt;p&gt;
    This analysis documents the techniques involved in predicting which team will win in a ranked League of Legends game. Ranked games consist of two teams of five players that try to win by killing the opposing team's nexus. To learn more about League of Legends, &lt;a href="http://leagueoflegends.com" target="_blank"&gt;follow this link&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
    Predicting the winner for the game League of Legends is a difficult task. There are many aspects of the game that must be considered. Player skill level, champion strength and Internet connectivity issues are just a few examples. The most accurate predictor to date is the summed total of each team's players champion win rate.
&lt;/p&gt;

&lt;h3&gt;
    Methods
&lt;/h3&gt;

&lt;h4&gt;
    Introduction
&lt;/h4&gt;

&lt;p&gt;
    Riot Games' League of Legends API was used to collect data for this analysis. The API provided endpoints for fetching player information, player match history and detailed match information. In addition, &lt;a href="http://www.cs.waikato.ac.nz/ml/weka/" target="_blank"&gt;Weka&lt;/a&gt; was used for data analysis and the creation of the predictive model. Several methods were used prior to the one discussed in this article, however they were left out for brevity.
&lt;/p&gt;

&lt;h4&gt;
    Data Collection
&lt;/h4&gt;

&lt;p&gt;
    Approximately 1,000 games from each League of Legends ranking system (bronze through challenger) was collected for training and testing the model. Data collection consisted of querying the API for players from each rank and finding that player's previous 20 matches. In some cases more data was collected than others. For example, there are significantly more players in the bronze tier than the challenger tier. 
&lt;/p&gt;

&lt;p&gt;
    While querying the API for each match, several sub-API calls were made to obtain each player's win rate for that champion. Once each player's win rates were collected, the totals were summed for each team. In the event that a player has less than 10 games on that particular champion; a win rate of 50% was assigned. Each team's total win rate never exceeds 500 percent (5 players per team with a maximum of 100% win rate per player).
&lt;/p&gt;

&lt;p&gt;
    A single entry would look like the following:
&lt;/p&gt;

&lt;table border="1" cellpadding="1" cellspacing="0"&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;
                &lt;strong&gt;BlueTeam&lt;/strong&gt;
            &lt;/td&gt;
            &lt;td&gt;
                &lt;strong&gt;RedTeam&lt;/strong&gt;
            &lt;/td&gt;
            &lt;td&gt;
                &lt;strong&gt;Winner&lt;/strong&gt;
            &lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;
                385
            &lt;/td&gt;
            &lt;td&gt;
                407
            &lt;/td&gt;
            &lt;td&gt;
                red
            &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4&gt;
    Data Analysis
&lt;/h4&gt;

&lt;p&gt;
    Initially, the data analysis process consisted of training and testing the model from the higher ranked match data (masters and challenger) using Weka. A number of different algorithms were used in the model: decision trees, baysian network and rule based. After testing all of the different models, the J48 decision tree algorithm performed the best (with adjustments to pruning etc).
&lt;/p&gt;

&lt;p&gt;
    Here are the J48 options used:
&lt;/p&gt;

&lt;pre&gt;
&lt;code class="language-bash"&gt;weka.classifiers.trees.J48 -R -N 5 -Q 1 -B -M 2 -A&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;
    With these options and using the challenger data set the model was able to successfully predict 90% of the games correctly. Testing the model against lower ranked matches showed that the model became more accurate when testing against lower tier data. For example, the master tier has an accuracy of 94% and the lowest tier (bronze) has an accuracy of 98%.
&lt;/p&gt;

&lt;h4&gt;
    Predictive Model Implementation
&lt;/h4&gt;

&lt;p&gt;
    The output from the Weka J48 decision tree algorithm was implemented in the Go programming language. You can see the full implementation below.
&lt;/p&gt;

&lt;pre&gt;
&lt;code class="language-go"&gt;package classifiers

/*
=== Run information ===

Scheme:weka.classifiers.trees.J48 -R -N 5 -Q 1 -B -M 2 -A
Relation:     team_win_rates_masters2
Instances:    3304
Attributes:   3
              BlueTeam
              RedTeam
              Win
Test mode:user supplied test set:     641instances

=== Classifier model (full training set) ===

J48 pruned tree
------------------

RedTeam &amp;lt;= 285.871643
|   BlueTeam &amp;lt;= 253.400162
|   |   RedTeam &amp;lt;= 258.712128
|   |   |   BlueTeam &amp;lt;= 229.873032: RedTeam (33.0/5.0)
|   |   |   BlueTeam &amp;gt; 229.873032: BlueTeam (38.0/13.0)
|   |   RedTeam &amp;gt; 258.712128: RedTeam (146.0/10.0)
|   BlueTeam &amp;gt; 253.400162
|   |   RedTeam &amp;lt;= 245.268372: BlueTeam (556.0/6.0)
|   |   RedTeam &amp;gt; 245.268372
|   |   |   BlueTeam &amp;lt;= 302.380981
|   |   |   |   BlueTeam &amp;lt;= 265.680847: RedTeam (53.0/18.0)
|   |   |   |   BlueTeam &amp;gt; 265.680847: BlueTeam (262.0/90.0)
|   |   |   BlueTeam &amp;gt; 302.380981: BlueTeam (286.0/18.0)
RedTeam &amp;gt; 285.871643
|   BlueTeam &amp;lt;= 284.519928: RedTeam (954.0/50.0)
|   BlueTeam &amp;gt; 284.519928
|   |   RedTeam &amp;lt;= 316.890594: BlueTeam (196.0/62.0)
|   |   RedTeam &amp;gt; 316.890594
|   |   |   BlueTeam &amp;lt;= 324.080078: RedTeam (101.0/6.0)
|   |   |   BlueTeam &amp;gt; 324.080078
|   |   |   |   BlueTeam &amp;lt;= 344.303864
|   |   |   |   |   RedTeam &amp;lt;= 330.416656: BlueTeam (7.0/2.0)
|   |   |   |   |   RedTeam &amp;gt; 330.416656: RedTeam (7.0)
|   |   |   |   BlueTeam &amp;gt; 344.303864: BlueTeam (5.0)

Number of Leaves  :     13

Size of the tree :  25


Time taken to build model: 0.03 seconds

=== Evaluation on test set ===
=== Summary ===

Correctly Classified Instances         603               94.0718 %
Incorrectly Classified Instances        38                5.9282 %
Kappa statistic                          0.8814
Mean absolute error                      0.1043
Root mean squared error                  0.2121
Relative absolute error                 20.856  %
Root relative squared error             42.3594 %
Total Number of Instances              641

=== Detailed Accuracy By Class ===

               TP Rate   FP Rate   Precision   Recall  F-Measure   ROC Area  Class
                 0.956     0.075      0.927     0.956     0.942      0.975    BlueTeam
                 0.925     0.044      0.955     0.925     0.94       0.975    RedTeam
Weighted Avg.    0.941     0.059      0.941     0.941     0.941      0.975

=== Confusion Matrix ===

   a   b   &amp;lt;-- classified as
 306  14 |   a = BlueTeam
  24 297 |   b = RedTeam
*/

// This classifies which league of legends team will win based on the
// summation of each player's specific champion win rate for their team.
// This is the implementation of the J48 decision tree's output. Overall this
// classifier has an accuracy of rougly 92%.
// The output of this classifier is a string of "blue" or "red".
func CWRWinningTeamClassifier(blueWinRate float64, redWinRate float64) string {
    var winner string
    if redWinRate &amp;lt;= 285.871643 {
        if blueWinRate &amp;lt;= 253.400162 {
            if redWinRate &amp;lt;= 258.712128 {
                if blueWinRate &amp;lt;= 229.873032 {
                    winner = "red"
                } else {
                    winner = "blue"
                }
            } else {
                winner = "red"
            }
        } else {
            if redWinRate &amp;lt;= 245.268372 {
                winner = "blue"
            } else {
                if blueWinRate &amp;lt;= 302.380981 {
                    if blueWinRate &amp;lt;= 265.680847 {
                        winner = "red"
                    } else {
                        winner = "blue"
                    }
                } else {
                    winner = "blue"
                }
            }
        }
    } else {
        if blueWinRate &amp;lt;= 284.519928 {
            winner = "red"
        } else {
            if redWinRate &amp;lt;= 316.890594 {
                winner = "blue"
            } else {
                if blueWinRate &amp;lt;= 324.080078 {
                    winner = "red"
                } else {
                    if blueWinRate &amp;lt;= 344.303864 {
                        if redWinRate &amp;lt;= 330.416656 {
                            winner = "blue"
                        } else {
                            winner = "red"
                        }
                    } else {
                        winner = "blue"
                    }
                }
            }
        }
    }
    return winner
}&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;
    Conclusion
&lt;/h3&gt;

&lt;p&gt;
    In conclusion, the best predictor for determining the winner of ranked League of Legends games seems to be the champion win rate. Using the model at any rank gives a very accurate prediction of 90% to 98%. If you are interested in using this implementation, then please visit &lt;a href="http://loldestiny.tylermarrs.com" target="_blank"&gt;loldestiny.tylermarrs.com&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;&lt;/div&gt;</description><category>data analytics</category><category>data science</category><category>league of legends</category><category>predictive modeling</category><guid>http://tylermarrs.com/posts/ranked-win-predictor/</guid><pubDate>Mon, 04 Jul 2016 02:26:22 GMT</pubDate></item><item><title>Challenger Tier Baron Throw Analysis</title><link>http://tylermarrs.com/posts/challenger-tier-baron-throw-analysis/</link><dc:creator>Tyler Marrs</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;
    Abstract
&lt;/h3&gt;
&lt;p&gt;
    This is a data analysis exercise to answer the question of: Which region throws the most at baron? If you are not familiar with the game, &lt;a href="http://leageoflegends.com" target="_blank"&gt;League of Legends&lt;/a&gt;, it is a &lt;a href="https://en.wikipedia.org/wiki/Multiplayer_online_battle_arena" target="_blank"&gt;Multiplayer Online Battle Arena (MOBA)&lt;/a&gt;. In short, the game consists of two teams that try to destroy one another's nexus. Each team consists of five players and each player gets to choose a champion to play as.
&lt;/p&gt;

&lt;div style="text-align: center;float:right"&gt;
    &lt;img alt="" src="http://vignette3.wikia.nocookie.net/leagueoflegends/images/5/5b/Baron_Nashor_VU.jpg" style="margin: 5px; width: 200px; height: 150px;"&gt;&lt;br&gt;
    &lt;span style="font-size:12px;"&gt;&lt;strong&gt;&lt;em&gt;Figure 1&lt;/em&gt;. Baron Nashor&lt;br&gt;
    Source &lt;a href="http://leagueoflegends.com" target="_blank"&gt;leagueoflegends.com&lt;/a&gt;&lt;/strong&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;p&gt;
    Although destroying the nexus is the winning objective, there are many small objectives within the game. Some of these objectives consist of killing monsters. Baron Nashor (Figure 1) is the toughest monster to kill within the game and on occassion poor choices by players causes a Baron Throw. A Baron Throw can be defined as a poor choice in an attempt to kill Baron Nashor that causes your team to die. Within my analysis I found that there is not a major difference between the challenger players within different regions. However, there is enough of a difference to show that some regions do throw at baron more than others by a small margin.
&lt;/p&gt;

&lt;h3&gt;
    Methods
&lt;/h3&gt;

&lt;h4&gt;
    Introduction
&lt;/h4&gt;

&lt;p&gt;
    Riot Games' League of Legends &lt;a href="https://developer.riotgames.com" target="_blank"&gt;API&lt;/a&gt; was used to collect data for this analysis. The API provided endpoints for fetching challenger players, summoner matches and match information. The overall process consisted of fetching data from the API, classifying matches for Baron Throws and finally visualization of the data.
&lt;/p&gt;

&lt;h4&gt;
    Data Collection
&lt;/h4&gt;

&lt;p&gt;
    Several scripts were created in the process of collecting the data. &lt;a href="https://bitbucket.org/snakemake/snakemake/wiki/Home" target="_blank"&gt;Snakemake&lt;/a&gt;, a Python workflow engine, was used to help streamline the process. The benefit to using Snakemake is that it enables you to resume the workflow from where it left off in the event of a software bug, power outage or hardware failure. Be aware that Snakemake is not magic. You must create your data pipeline in a way that it can be resumed in the event of a failure.
&lt;/p&gt;

&lt;div style="text-align: center;"&gt;
    &lt;img alt="baron throws data pipeline" class="alignnone size-full wp-image-44" height="214" src="http://tylermarrs.com/wp-content/uploads/2016/06/baron-throws-data-pipeline.png" width="734"&gt;&lt;br&gt;
    &lt;span style="font-size:12px;"&gt;&lt;strong&gt;&lt;em&gt;Figure 2&lt;/em&gt;. Snakemake Data Pipeline&lt;/strong&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;p&gt;
    The first step was to query for the players within the challenger tier for all 11 regions (Table 1). As of the year 2016, each region contains 200 challenger tier players. Analyzing every match that a player has participated in would be exhaustive. For example, some players started playing the game when it was first created in 2009. Assuming the player only played on that account, it could be thousands of games to analyze. To narrow down match history, the matches played during the 2016 season were collected for each player. On average this consisted of 335 games per player.
&lt;/p&gt;

&lt;table align="center" border="1" cellpadding="5" cellspacing="0"&gt;
    &lt;caption&gt;
        &lt;span style="font-size:12px;"&gt;&lt;strong&gt;&lt;em&gt;Table 1&lt;/em&gt;. Regions&lt;/strong&gt;&lt;/span&gt;
    &lt;/caption&gt;
    &lt;tbody&gt;
        &lt;tr style="background-color:#CCC"&gt;
            &lt;td&gt;
                &lt;strong&gt;Region&lt;/strong&gt;
            &lt;/td&gt;
            &lt;td&gt;
                &lt;strong&gt;Code&lt;/strong&gt;
            &lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;
                Brazil
            &lt;/td&gt;
            &lt;td&gt;
                BR
            &lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;
                Latin America North
            &lt;/td&gt;
            &lt;td&gt;
                LAN
            &lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;
                Oceania
            &lt;/td&gt;
            &lt;td&gt;
                OCE
            &lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;
                Turkish
            &lt;/td&gt;
            &lt;td&gt;
                TR
            &lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;
                Latin America South
            &lt;/td&gt;
            &lt;td&gt;
                LAS
            &lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;
                Russia
            &lt;/td&gt;
            &lt;td&gt;
                RU
            &lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;
                European Union West
            &lt;/td&gt;
            &lt;td&gt;
                EUW
            &lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;
                Japan
            &lt;/td&gt;
            &lt;td&gt;
                JP
            &lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;
                Korea
            &lt;/td&gt;
            &lt;td&gt;
                KR
            &lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;
                North America
            &lt;/td&gt;
            &lt;td&gt;
                NA
            &lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;
                European Union Northeast
            &lt;/td&gt;
            &lt;td&gt;
                EUNE
            &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
    The longest process in the data pipeline is during the match fetching phase. This is due to the massive number of API requests that need to be made; one for each match. Approximately 750,000 API calls to fetch match information was required. During each API call consideration of rate limiting and service instability must be handled. Once the match is fetched it is classified as a Baron Throw or not and written to a flat file so that it can be aggregated for data visualization. Data aggregation consists of grouping each player's matches from each region to determine the number of Baron Throws that occurred within their match history. The Baron Throw rate is a simple division of Baron Throws / matches played. Furthermore each aggregated result is dumped in JSON format and uploaded to the web server so that it can be visualized. The web page makes use of a JavaScript plotting library; &lt;a href="https://plot.ly/javascript/" target="_blank"&gt;Plotly.js&lt;/a&gt;.
&lt;/p&gt;

&lt;h4&gt;
    Baron Throw Classification
&lt;/h4&gt;

&lt;p&gt;
    In order to determine if a baron throw really occurred, time series analysis and positional analysis was required. Without positional analysis it would be difficult to know for sure if a Baron throw really occurred. This is due to the location of Baron being relatively small compared to the entire game map. So only looking at deaths within the timeframe of the Baron kill event may not be very accurate.
&lt;/p&gt;

&lt;div style="text-align: center;"&gt;
    &lt;img alt="Baron zone" class="alignnone size-full wp-image-30" height="515" src="http://tylermarrs.com/wp-content/uploads/2016/06/baron_zone.png" width="515"&gt;&lt;br&gt;
    &lt;span style="font-size:12px;"&gt;&lt;strong&gt;&lt;em&gt;Figure 3&lt;/em&gt;. Summoner's Rift Baron Throw Zone&lt;/strong&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;p&gt;
    The match data provided by Riot includes events within the form of a time series and XY coordinates of where the event occurred. Purely analyzing kill events within a given time span is not sufficient to determine if a Baron Throw event occurred. For example, a small group of players could be killed at the bottom right corner of the map while baron was taken successfully by other players. To make the classifier as accurate as possible, a radius around Baron Nashor was created (Figure 3) and used to determine if player deaths coincided within this area. Since Baron Nashor does not spawn until 20 minutes into the game, the search space could be narrowed down to all events after 20 minutes. Once a Baron Nashor kill event was found, events within a 30 second period before and after is analyzed for player deaths. If 4 or 5 players were killed on the same team within the baron zone and within the thresholds of 30 seconds before the baron event or 30 seconds after the baron event, a Baron Throw occurred. Figure 4 shows the classification algorithm in pseudocode form.
&lt;/p&gt;

&lt;div&gt;
    &lt;pre&gt;
&lt;code class="language-go"&gt;BARON_X = 5007
BARON_Y = 10471
BARON_R = 1947

function inBaronZone(x, y)
 distance_x = BARON_X - x
 distance_y = BARON_Y - y
 square_dist = (distance_x * distance_x) + (distance_y * distance_y)
 square_r = BARON_R * BARON_R

 return square_dist &amp;lt;= square_r
end

function hasBaronKills(match)
  match.BaronKills &amp;gt; 0
end

function playerTeam(match, summoner_id)
  return blue or red accordingly
end

KillEvent class {
  KillerID integer
  ParticipantID integer
  X integer
  Y integer
}

BaronEvent class {
  FrameIndex integer
  Time integer
  KillerID integer
  KillEvents array
}

function isBaronThrow(match, summoner)
  if not hasBaronKills(match)
    return false
  end

  baron_events = array
  for each index, timeline event
    if timeline event &amp;lt; 20 minutes
      continue
    end

    if timeline event == baron kill
      create baron event instance with values
      add baron event to baron_events array
    end
  end

  player_team = playerTeam(match, summoner_id)

  // Look at one frame before and after the baron event for kills
  for each baron_events
    for each timeevent in range of baron_event.FrameIndex -1 to baron_event.FrameIndex + 1
      if kill event and event 30 seconds before baron or 30 seconds after baron
        create kill event instance with values
        add kill event to baron event
      end
    end
  end

  is_throw = false
  for each baron_events
    blue_deaths = 0
    red_deaths = 0
    for each baron_event.kill_events
      if inBaronZone(kill_event.x, kill_event.y)
        if kill_event.ParticipantID is blue team
          add 1 to blue_deaths
        else
          add 1 to red_deaths
        end
      end

      if player_team == blue &amp;amp;&amp;amp; blue_deaths &amp;gt;= 4 or player_team == red and red_deaths &amp;gt;= 4
        is_throw = true
        break from loop
      end
    end
  end
  return is_throw
end&lt;/code&gt;&lt;/pre&gt;

    &lt;p style="text-align: center;"&gt;
        &lt;span style="font-size:12px;"&gt;&lt;strong&gt;&lt;em&gt;Figure 4&lt;/em&gt;. Baron Throw Classifier Pseudocode&lt;/strong&gt;&lt;/span&gt;
    &lt;/p&gt;
&lt;/div&gt;

&lt;h4&gt;
    Conclusion
&lt;/h4&gt;

&lt;div style="text-align: center;t"&gt;
    &lt;img alt="Challenger tier baron throws by region" class="alignnone size-full wp-image-31" height="500" src="http://tylermarrs.com/wp-content/uploads/2016/06/challenger_region_baron_throws.png" width="1000"&gt;&lt;br&gt;
    &lt;span style="font-size:12px;"&gt;&lt;strong&gt;&lt;em&gt;Figure 5&lt;/em&gt;. Challenger Throw Rate by Region&lt;/strong&gt; &lt;/span&gt;
&lt;/div&gt;

&lt;p&gt;
    While a significant margin is not illustrated in Figure 5, a small difference of Baron Throws is observed across each region. Korea shows the highest median rate of 1.9% for baron throws and Latin America South shows the lowest rate of 1.23% for Baron Throws. The region in which a player plays League of Legends does not seem to be correlated with the Baron Throw rate. The small differences amongst regions may indicate the importance placed on Baron Nashor between these regions, however further analysis would need to be performed. A better observation for Baron Throw analysis could consist of comparisons amongst player ranks within each region. The player rank, according to Riot, illustrates the skill level of a player. Analyzing the throw rate of a player in the lowest tier, bronze, against the highest tier, challenger, should show a significant difference.
&lt;/p&gt;

&lt;p&gt;
    To see up to date and more graphs at the regional level, please visit &lt;a href="http://lolstats.tylermarrs.com/baronthrows" target="_blank"&gt;lolstats.tylermarrs.com/baronthrows&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;&lt;/div&gt;</description><category>automation</category><category>data analytics</category><category>data classification</category><category>data visualization</category><category>league of legends</category><guid>http://tylermarrs.com/posts/challenger-tier-baron-throw-analysis/</guid><pubDate>Fri, 10 Jun 2016 04:59:31 GMT</pubDate></item></channel></rss>